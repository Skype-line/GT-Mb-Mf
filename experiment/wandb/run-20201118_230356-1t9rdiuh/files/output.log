****** begin! ******
****** step1 ******
/home/yunke/git/DL_MBMF/experiment/MVE_agent.py:115: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  s = torch.from_numpy(np.vstack((t.s for t in transitions if t is not None))).float().to(self.device)
transition loss: 5.024040222167969
reward loss: 42.56182098388672
actor loss: -0.08052321523427963
critic loss: 2.3813014030456543
episode 1, total reward -466.93439210913215
transition loss: 4.49483585357666
reward loss: 46.28855895996094
actor loss: 0.025178469717502594
critic loss: 2.667466163635254
episode 2, total reward -471.8969802169275
transition loss: 3.1916112899780273
reward loss: 47.60071563720703
actor loss: 0.12949542701244354
critic loss: 2.905555248260498
episode 3, total reward -636.7535723824691
transition loss: 3.9148237705230713
reward loss: 45.745880126953125
actor loss: 0.2562520205974579
critic loss: 3.110914707183838
episode 4, total reward -376.0401229311926
transition loss: 2.882261037826538
reward loss: 48.512229919433594
actor loss: 0.3278195858001709
critic loss: 3.3365516662597656
episode 5, total reward -598.9063895353235
transition loss: 2.1587212085723877
reward loss: 37.75501251220703
actor loss: 0.39619067311286926
critic loss: 2.9521632194519043
episode 6, total reward -474.6665487852937
transition loss: 2.4876949787139893
reward loss: 39.1866455078125
actor loss: 0.534190833568573
critic loss: 3.4333808422088623
episode 7, total reward -469.0072405172533
transition loss: 2.267669677734375
reward loss: 38.044864654541016
actor loss: 0.6302764415740967
critic loss: 3.62030291557312
episode 8, total reward -361.3685247759322
transition loss: 2.0026190280914307
reward loss: 32.73426055908203
actor loss: 0.7034922242164612
critic loss: 3.5616230964660645
episode 9, total reward -577.896303195485
transition loss: 2.381380796432495
reward loss: 35.81394958496094
actor loss: 0.9040442109107971
critic loss: 4.2899980545043945
episode 10, total reward -446.75969614835356
transition loss: 1.5679244995117188
reward loss: 31.569446563720703
actor loss: 0.8997042179107666
critic loss: 3.963599681854248
episode 11, total reward -555.2853101396536
transition loss: 1.8623217344284058
reward loss: 34.389892578125
actor loss: 1.064892053604126
critic loss: 4.792329788208008
episode 12, total reward -647.0397976599427
transition loss: 1.4755662679672241
reward loss: 33.7072639465332
actor loss: 1.1844639778137207
critic loss: 5.2658467292785645
episode 13, total reward -430.64917197210485
transition loss: 1.2156493663787842
reward loss: 26.044153213500977
actor loss: 1.2047089338302612
critic loss: 4.979109764099121
episode 14, total reward -462.98187221012165
transition loss: 1.6084752082824707
reward loss: 24.505329132080078
actor loss: 1.5664979219436646
critic loss: 6.085195064544678
episode 15, total reward -262.79842362470896
transition loss: 1.1938731670379639
reward loss: 24.250041961669922
actor loss: 1.5531049966812134
critic loss: 6.330046653747559
episode 16, total reward -559.1742269942264
transition loss: 1.2116471529006958
reward loss: 21.855207443237305
actor loss: 1.7708759307861328
critic loss: 7.29449462890625
episode 17, total reward -471.85233541077645
transition loss: 1.0209892988204956
reward loss: 22.84541130065918
actor loss: 1.8538107872009277
critic loss: 8.255253791809082
episode 18, total reward -458.63581982891765
transition loss: 1.0602869987487793
reward loss: 21.586326599121094
actor loss: 2.134833335876465
critic loss: 10.013345718383789
episode 19, total reward -463.246501523688
episode 20, total reward -285.52035244660635Traceback (most recent call last):

start test!
  File "MBMF.py", line 151, in <module>
    main(conf)
  File "MBMF.py", line 123, in main
    test_action = agent.select_action(j, state_list[j], exploration=False)[:,0]
TypeError: select_action() got multiple values for argument 'exploration'
