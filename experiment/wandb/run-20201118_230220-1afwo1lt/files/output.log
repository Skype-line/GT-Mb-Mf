****** begin! ******
****** step1 ******
/home/yunke/git/DL_MBMF/experiment/MVE_agent.py:115: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  s = torch.from_numpy(np.vstack((t.s for t in transitions if t is not None))).float().to(self.device)
transition loss: 6.001134395599365
reward loss: 34.51228332519531
actor loss: -0.1386776715517044
critic loss: 1.7820402383804321
episode 1, total reward -309.1726462046775
transition loss: 5.596086502075195
reward loss: 39.50263595581055
actor loss: -0.012058677151799202
critic loss: 2.2160868644714355
episode 2, total reward -477.712125305816
transition loss: 4.179773807525635
reward loss: 40.16988754272461
actor loss: 0.09753812849521637
critic loss: 2.4660167694091797
episode 3, total reward -533.7804412318296
transition loss: 5.230863094329834
reward loss: 44.84540939331055
actor loss: 0.23354604840278625
critic loss: 2.9429235458374023
episode 4, total reward -364.53121627700887
transition loss: 4.651337146759033
reward loss: 39.070003509521484
actor loss: 0.3476998805999756
critic loss: 2.9578607082366943
episode 5, total reward -486.40068907375024
transition loss: 4.2976555824279785
reward loss: 34.53801345825195
actor loss: 0.4353213310241699
critic loss: 3.0010197162628174
episode 6, total reward -360.389510017246
transition loss: 4.652903079986572
reward loss: 34.916194915771484
actor loss: 0.6039485335350037
critic loss: 3.293389081954956
episode 7, total reward -452.8368671132302
transition loss: 3.548417806625366
reward loss: 28.922119140625
actor loss: 0.6733283996582031
critic loss: 3.082875967025757
episode 8, total reward -471.90360751620597
transition loss: 3.6078615188598633
reward loss: 31.30442237854004
actor loss: 0.8229884505271912
critic loss: 3.74104380607605
episode 9, total reward -444.69590382362236
transition loss: 3.2679977416992188
reward loss: 31.90789031982422
actor loss: 0.8888463973999023
critic loss: 3.9126381874084473
episode 10, total reward -642.5123120971418
transition loss: 3.232497215270996
reward loss: 31.998550415039062
actor loss: 1.0328553915023804
critic loss: 4.322964191436768
episode 11, total reward -363.41874198670365
transition loss: 2.456223249435425
reward loss: 29.241466522216797
actor loss: 1.1424226760864258
critic loss: 4.215366840362549
episode 12, total reward -576.976162811384
transition loss: 2.5399746894836426
reward loss: 31.770008087158203
actor loss: 1.26371431350708
critic loss: 5.119091987609863
episode 13, total reward -519.3402378055005
transition loss: 2.276493787765503
reward loss: 27.073318481445312
actor loss: 1.4108458757400513
critic loss: 5.2968645095825195
episode 14, total reward -460.4263953573704
transition loss: 2.010575532913208
reward loss: 22.560659408569336
actor loss: 1.4008746147155762
critic loss: 5.608264923095703
episode 15, total reward -541.2326154430515
transition loss: 1.4606224298477173
reward loss: 20.970558166503906
actor loss: 1.387614130973816
critic loss: 5.552877902984619
episode 16, total reward -635.7418115701599
transition loss: 1.6878248453140259
reward loss: 24.608722686767578
actor loss: 1.7743570804595947
critic loss: 6.83251953125
episode 17, total reward -648.4119816638556
transition loss: 1.325659155845642
reward loss: 24.603317260742188
actor loss: 1.770700454711914
critic loss: 7.363171577453613
episode 18, total reward -476.37578546418854
transition loss: 1.1564679145812988
reward loss: 20.610239028930664
actor loss: 1.973520278930664
critic loss: 8.407631874084473
episode 19, total reward -374.39483537585784
episode 20, total reward -671.5723119705494
Traceback (most recent call last):
start test!
  File "MBMF.py", line 151, in <module>
    main(conf)
  File "MBMF.py", line 123, in main
    test_action = agent.select_action(j, state_list[j], exploration=1)[:,0]
TypeError: select_action() got multiple values for argument 'exploration'
