****** begin! ******
****** step1 ******
/home/yunke/git/DL_MBMF/experiment/MVE_agent.py:115: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  s = torch.from_numpy(np.vstack((t.s for t in transitions if t is not None))).float().to(self.device)
transition loss: 5.211585998535156
reward loss: 37.439327239990234
actor loss: 0.15877526998519897
critic loss: 1.5944321155548096
episode 1, total reward -375.5167878127603
transition loss: 5.640712738037109
reward loss: 38.59186935424805
actor loss: 0.30591824650764465
critic loss: 1.798232078552246
episode 2, total reward -333.76919202709803
transition loss: 5.546407699584961
reward loss: 35.989994049072266
actor loss: 0.42645230889320374
critic loss: 1.8854039907455444
episode 3, total reward -358.2486282372237
transition loss: 5.302574157714844
reward loss: 34.889244079589844
actor loss: 0.5874108076095581
critic loss: 2.0314011573791504
episode 4, total reward -374.711802073009
transition loss: 4.439180850982666
reward loss: 31.996570587158203
actor loss: 0.7114492058753967
critic loss: 2.0976898670196533
episode 5, total reward -411.29672088268217
transition loss: 4.114677906036377
reward loss: 31.912277221679688
actor loss: 0.8247986435890198
critic loss: 2.333451509475708
episode 6, total reward -386.6308299924167
transition loss: 3.9875030517578125
reward loss: 26.219011306762695
actor loss: 1.00065016746521
critic loss: 2.3948428630828857
episode 7, total reward -466.1469543183523
transition loss: 3.171649932861328
reward loss: 27.255102157592773
actor loss: 1.102941632270813
critic loss: 2.753405809402466
episode 8, total reward -360.45323416308923
transition loss: 2.5065572261810303
reward loss: 20.864032745361328
actor loss: 1.2559094429016113
critic loss: 2.7137792110443115
episode 9, total reward -396.19812687031384
transition loss: 2.5003602504730225
reward loss: 21.42396354675293
actor loss: 1.405797004699707
critic loss: 3.2466092109680176
episode 10, total reward -483.16718443741075
transition loss: 2.805835723876953
reward loss: 30.99616241455078
actor loss: 1.7369496822357178
critic loss: 5.006872653961182
episode 11, total reward -362.0320065928909
transition loss: 2.4800827503204346
reward loss: 20.07048988342285
actor loss: 1.8149067163467407
critic loss: 4.739544868469238
episode 12, total reward -367.6265614110626
transition loss: 1.823671817779541
reward loss: 18.985374450683594
actor loss: 1.9584341049194336
critic loss: 5.496182918548584
episode 13, total reward -503.0899628298
transition loss: 1.5524088144302368
reward loss: 15.542598724365234
actor loss: 2.0642802715301514
critic loss: 6.039734840393066
episode 14, total reward -512.0954943480682
transition loss: 1.7418793439865112
reward loss: 18.277603149414062
actor loss: 2.4590611457824707
critic loss: 8.590755462646484
episode 15, total reward -364.5829832150164
transition loss: 1.235500693321228
reward loss: 15.458549499511719
actor loss: 2.380506992340088
critic loss: 8.737162590026855
episode 16, total reward -517.0260956256635
transition loss: 1.0489635467529297
reward loss: 14.256864547729492
actor loss: 2.617095947265625
critic loss: 11.150079727172852
episode 17, total reward -465.7120372051379
transition loss: 1.016438364982605
reward loss: 10.866588592529297
actor loss: 2.920790672302246
critic loss: 13.888972282409668
episode 18, total reward -335.15377863593966
transition loss: 0.9834990501403809
reward loss: 10.426238059997559
actor loss: 3.373000144958496
critic loss: 18.87840461730957
episode 19, total reward -388.31710552583075
episode 20, total reward -616.5326214645248
start test!
Traceback (most recent call last):
  File "MBMF.py", line 151, in <module>
    main(conf)
  File "MBMF.py", line 123, in main
    test_action = agent.select_action(step_num, test_state_list[step_num], False)[:,0]
TypeError: select_action() takes from 2 to 3 positional arguments but 4 were given
